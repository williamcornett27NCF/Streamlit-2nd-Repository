## 1. File hashing to avoid duplicate resumes
# utils/file_hash.py
import hashlib

def compute_file_hash(file_bytes: bytes) -> str:
    hasher = hashlib.sha256()
    hasher.update(file_bytes)
    return hasher.hexdigest()

# In app.py after upload:
from utils.file_hash import compute_file_hash
from db_utils import get_resume_by_hash, insert_resume

uploaded_file = st.file_uploader("Upload resume", type=["pdf", "docx"])

if uploaded_file is not None:
    file_bytes = uploaded_file.read()
    file_hash = compute_file_hash(file_bytes)

    existing = get_resume_by_hash(file_hash)
    if existing:
        st.warning("This resume is already in the system.")
    else:
        # parse, extract, insert
        ...

## 2. Real parse_resume dispatcher with Python code
# resume_parser.py
from typing import Literal

from PyPDF2 import PdfReader
import docx

def extract_text_from_pdf(file_bytes: bytes) -> str:
    reader = PdfReader(io.BytesIO(file_bytes))
    text = []

## Simple Streamlit UI for match score + suggestions
# app.py â€“ inside "Resume Analysis" tab

selected_job_id = st.selectbox("Select job", [j["id"] for j in jobs])
selected_job = next(j for j in jobs if j["id"] == selected_job_id)

if st.button("Analyze resume vs job"):
    analysis = analyze_resume_against_job(resume, selected_job)

    st.subheader("Match Score")
    st.progress(int(analysis.match_score))
    st.write(f"{analysis.match_score}% skill match")

    st.subheader("Matching skills")
    st.write(", ".join(analysis.matching_skills) or "None found")

    st.subheader("Missing skills")
    st.write(", ".join(analysis.missing_skills) or "None")

    st.subheader("Keyword suggestions")
    st.write(", ".join(analysis.keyword_suggestions) or "None")
    for page in reader.pages:
        text.append(page.extract_text() or "")
    return "\n".join(text)

def extract_text_from_docx(file_bytes: bytes) -> str:
    doc = docx.Document(io.BytesIO(file_bytes))
    paragraphs = [p.text for p in doc.paragraphs]
    return "\n".join(paragraphs)

def parse_resume(file_bytes: bytes, file_type: Literal["pdf", "docx"]) -> str:
    if file_type == "pdf":
        return extract_text_from_pdf(file_bytes)
    elif file_type == "docx":
        return extract_text_from_docx(file_bytes)
    else:
        raise ValueError(f"Unsupported file type: {file_type}")

# In app.py:
import mimetypes
from resume_parser import parse_resume

if uploaded_file:
    mime, _ = mimetypes.guess_type(uploaded_file.name)
    if mime == "application/pdf":
        file_type = "pdf"
    else:
        file_type = "docx"   # naive but works for v1

    raw_text = parse_resume(file_bytes, file_type)
    st.text_area("Extracted text (preview)", raw_text[:3000])

## 3. Simple skills matching + match score
# analysis.py
from typing import List, Tuple, Set

def normalize_skill(s: str) -> str:
    return s.strip().lower()

def compare_skills(resume_skills: List[str], job_skills: List[str]) -> Tuple[List[str], List[str], float]:
    r_set: Set[str] = {normalize_skill(s) for s in resume_skills}
    j_set: Set[str] = {normalize_skill(s) for s in job_skills}

    matching = sorted(r_set & j_set)
    missing = sorted(j_set - r_set)

    match_score = 0.0
    if j_set:
        match_score = round(100 * len(matching) / len(j_set), 2)

    return matching, missing, match_score
# Used in analyze_resume_against_job:
from models import AnalysisResult

def analyze_resume_against_job(resume: Resume, job: JobListing) -> AnalysisResult:
    job_skills = extract_skills_from_job(job)         # your existing function
    matching, missing, match_score = compare_skills(resume.skills, job_skills)

    return AnalysisResult(
        resume_id=resume.id,
        job_listing_id=job.id,
        match_score=match_score,
        matching_skills=matching,
        missing_skills=missing,
        keyword_suggestions=list(missing),  # simple v1
        improvement_suggestions=[],
        metadata={"source": "rule-based-v1"}
    )
